deepdive {

  sampler.sampler_args: "-l 300 -s 1 -i 500 --alpha 0.1 -c 0"

  schema.variables {
    locations.value: Categorical(144346)
  }

  pipeline.run: all 

  pipeline.pipelines {
    extract = [ extract_locations, extract_locations_features, supervise_locations ]
    infer = [ locations_features ]
    all = [ extract_locations, extract_locations_features, supervise_locations, locations_features ]
  }

  db.default {
    driver   : "org.postgresql.Driver"
    url      : "jdbc:postgresql://"${PGHOST}":"${PGPORT}"/"${DBNAME}
    user     : ${PGUSER}
    password : ${PGPASSWORD}
    dbname   : ${DBNAME}
    host     : ${PGHOST}
    port     : ${PGPORT}
    gphost   : ${GPHOST}
    gpport   : ${GPPORT}
    gppath   : ${GPPATH}
    # start gpfdist server on the machine running the application with
    # `rungpcommand 'gpfdist -d /lfs/raiders4/0/rionda/greenplum_gpfdist-memex -p 9999'`
  }

  calibration.holdout_fraction: 0.075

  extraction.extractors {

    extract_locations: {
        style: tsv_extractor
        before: psql -d ${DBNAME} -f ${APP_HOME}/schemas/locations.sql
        input: """
                SELECT
                        sent_id,
                        words,
                        poses
                FROM sentences;
                """
        output_relation: locations
        udf: ${APP_HOME}/udf/extract_locations.py
    }

    extract_locations_features: {
        style: tsv_extractor
        before: psql -d ${DBNAME} -f ${APP_HOME}/schemas/locations_features.sql
        input: """
                SELECT   c.mention_id,
                         s.sent_id,
                         s.words,
                         s.poses,
                         c.mention_num,
                         c.w_from,
                         c.w_to
                FROM sentences s, locations c
                WHERE s.sent_id = c.sent_id;
               """
         output_relation: locations_features
         udf: ${APP_HOME}/udf/extract_locations_features.py
         dependencies: [ extract_locations ]
    }

    supervise_locations: {
         style: tsv_extractor
         input: """
                 SELECT c.mention_id,
			c.sent_id,
			c.mention_num,
			c.mention_str,
			c.w_from,
			c.w_to,
			c.value
                 FROM locations c;"""
         output_relation: locations
         udf: ${APP_HOME}/udf/supervise_locations.py
         dependencies: [ extract_locations_features ]
    }
  }
  
  inference.factors {

    locations_features {
      input_query = """
        SELECT "locations.id", "locations.value", null as "features.id", "features.loc_id", "features.feature"
        FROM (
          SELECT c.id as "locations.id", c.value as "locations.value", f.loc_id as "features.loc_id", f.feature as "features.feature"
          FROM 
            locations c,
            locations_features f
          WHERE
            c.mention_id = f.mention_id
       ) s;
           """
      function: "Equal(locations.value, features.loc_id)" 
      weight: "?(features.feature)"
    }

    #factor_linear_chain_crf { 
    #  input_query: """
    #    SELECT l1.id as "locations.l1.id", l2.id as "locations.l2.id", 
    #       l1.value as "locations.l1.value", l2.value as "locations.l2.value" 
    #    FROM locations l1, locations l2 
    #    WHERE l1.sent_id = l2.sent_id AND l2.mention_num = l1.mention_num + 1""" 
    #  function: "Multinomial(locations.l1.value, locations.l2.value)" 
    #  weight: "?" 
    #}
  }
}

